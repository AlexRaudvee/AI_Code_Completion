[
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)",
        "middle": "def tokenize_lang_in(text):",
        "suffix": "    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):",
        "middle": "    return [tok.text for tok in spacy_lang_out.tokenizer(text)]",
        "suffix": "\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import spacy",
        "middle": "from torchtext.data import Field ",
        "suffix": "from torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):",
        "middle": "    return [tok.text for tok in spacy_lang_in.tokenizer(text)]",
        "suffix": "\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]",
        "middle": "def tokenize_lang_out(text):",
        "suffix": "    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n",
        "middle": "def tokenize_lang_out(text):",
        "suffix": "    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):",
        "middle": "    return [tok.text for tok in spacy_lang_in.tokenizer(text)]",
        "suffix": "\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field ",
        "middle": "from torchtext.datasets import Multi30k",
        "suffix": "\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import spacy",
        "middle": "from torchtext.data import Field ",
        "suffix": "from torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(",
        "middle": "    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'",
        "suffix": ")\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:",
        "middle": "        tokens = [token.lower() for token in sentence]",
        "suffix": "\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:",
        "middle": "        tokens = [token.text.lower() for token in spacy_ger(sentence)]",
        "suffix": "    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)",
        "middle": "",
        "suffix": "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor",
        "middle": "    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)",
        "suffix": "\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n",
        "middle": "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):",
        "suffix": "    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n",
        "middle": "        outputs.append(best_guess)",
        "suffix": "\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n",
        "middle": "    for _ in range(max_length):",
        "suffix": "        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]",
        "middle": "",
        "suffix": "def bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:",
        "middle": "        tokens = [token.lower() for token in sentence]",
        "suffix": "\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n",
        "middle": "def translate_sentence(model, sentence, german, english, device, max_length=50):",
        "suffix": "    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"",
        "middle": "for epoch in range(num_epochs):",
        "suffix": "    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot",
        "middle": "writer = SummaryWriter(f\"runs/loss_plot\")",
        "suffix": "step = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim",
        "middle": "import torch.nn as nn",
        "suffix": "from torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data",
        "middle": "from Net import Encoder, Decoder, EncDec, device",
        "suffix": "\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are",
        "middle": "        # within a healthy range",
        "suffix": "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n",
        "middle": "    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}",
        "suffix": "    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)",
        "middle": "decoder_net = Decoder(",
        "suffix": "    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's",
        "middle": "num_layers = 2",
        "suffix": "enc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).",
        "middle": "        output = output[1:].reshape(-1, output.shape[2])",
        "suffix": "        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)",
        "middle": "if load_model:",
        "suffix": "    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00c3\u00a4nnern darauf wird von einem gro\u00c3\u0178en pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model",
        "middle": "def check_accuracy(loader, model):",
        "suffix": "    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim",
        "middle": "from Net import RNN",
        "suffix": "from config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0",
        "middle": "    num_samples = 0",
        "suffix": "\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001",
        "middle": "batch_size = 64",
        "suffix": "num_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward ",
        "middle": "        scores = model(data)",
        "suffix": "        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:",
        "middle": "            x = x.to(device=device).squeeze(1)",
        "suffix": "            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)",
        "middle": "# Train network ",
        "suffix": "for epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)",
        "middle": "        # forward ",
        "suffix": "        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train",
        "middle": "    model.train()",
        "suffix": "    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN",
        "middle": "from config import device",
        "suffix": "from load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out",
        "middle": "from config import system",
        "suffix": "\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers",
        "middle": "        self.embedding = nn.Embedding(input_size, embedding_size)",
        "suffix": "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n",
        "middle": "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)",
        "suffix": "        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output",
        "middle": "            top1 = output.argmax(1)",
        "suffix": "            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)",
        "middle": "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)",
        "suffix": "\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n",
        "middle": "    def forward(self, x, hidden, cell):",
        "suffix": "        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size",
        "middle": "        self.num_layers = num_layers",
        "suffix": "\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n",
        "middle": "        self.embedding = nn.Embedding(input_size, embedding_size)",
        "suffix": "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):",
        "middle": "        batch_size = target.shape[1]",
        "suffix": "        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder",
        "middle": "        self.decoder = decoder",
        "suffix": "\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, ",
        "middle": "                self.hidden_size, ",
        "suffix": "                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)",
        "middle": "        return hidden, cell",
        "suffix": "    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n",
        "middle": "                self.rnn.zero_grad()",
        "suffix": "                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]",
        "middle": "            predicted_char = self.all_characters[top_char]",
        "suffix": "            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)",
        "middle": "        for i in range(self.batch_size):",
        "suffix": "            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])",
        "middle": "            text_target[i, :] = self.char_tensor(text_str[1:])",
        "suffix": "\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]",
        "middle": "        text_input = torch.zeros(self.batch_size, self.chunk_len)",
        "suffix": "        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n",
        "middle": "            for epoch in range(1, self.num_epochs + 1):",
        "suffix": "                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len",
        "middle": "                if epoch % self.print_every == 0:",
        "suffix": "                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n",
        "middle": "        initial_str = initial_str.lower()",
        "suffix": "\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers",
        "middle": "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)",
        "suffix": "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers",
        "middle": "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)",
        "suffix": "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys ",
        "middle": "import os",
        "suffix": "\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n",
        "middle": "# create RNN",
        "suffix": "class RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)",
        "middle": "        out = self.fc(out)",
        "suffix": ""
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size",
        "middle": "        self.num_layers = num_layers",
        "suffix": "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)",
        "middle": "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)",
        "suffix": "\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)",
        "middle": "        # forward prop ",
        "suffix": "        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out"
    },
    {
        "prefix": "import torch \nimport sys ",
        "middle": "import os",
        "suffix": "\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)",
        "middle": "        out = self.fc(out)",
        "suffix": "        \n        return out\n"
    }
]