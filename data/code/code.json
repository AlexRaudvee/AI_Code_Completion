[
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)",
        "middle": "        out = self.fc(out)",
        "suffix": "",
        "generated_middle": "\n        out = self.fc(out)\n        return out\n\n# create CNN\nclass CNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(CNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cnn = nn.Sequential(\n            nn.Conv2d(input_size, hidden_size, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.Linear(hidden_size*sequence_length, num_classes)\n        )\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.cnn(x)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        return out\n\n# create LSTM\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.lstm(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        return out\n\n# create GRU\nclass GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(GRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.gru(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        return out\n\n# create LSTM\nclass LSTM_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTM_2, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.lstm(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        return out\n\n# create GRU\nclass GRU_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(GRU_2, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.L"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):",
        "middle": "    def __init__(self, input_size, hidden_size, num_layers, num_classes):",
        "suffix": "        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n",
        "generated_middle": "\n        def __init__(self, input_size, hidden_size, num_layers, batch_first=False):\n            super(RNN, self).__init__()\n            self.hidden_size = hidden_size\n            self.num_layers = num_layers\n            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=batch_first)\n            self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, batch_first=False):\n        super(LSTM, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=batch_first)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, batch_first=False):\n        super(GRU, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=batch_first)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass LSTM_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, batch_first=False):\n        super(LSTM_2, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=batch_first)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass GRU_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, batch_first=False):\n        super(GRU_2, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=batch_first)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass LST"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)",
        "middle": "        out = self.fc(out)",
        "suffix": "        \n        return out\n",
        "generated_middle": "\n        out = self.fc(out)\n        return out\n\n# create CNN\nclass CNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(CNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Flatten(),\n            nn.Linear(hidden_size*sequence_length, num_classes)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)"
    },
    {
        "prefix": "import torch \nimport sys \nimport os",
        "middle": "import torch.nn as nn ",
        "suffix": "from config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out",
        "generated_middle": "\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)",
        "middle": "        out = self.fc(out)",
        "suffix": "        \n        return out\n",
        "generated_middle": "\n        out = self.fc(out)\n        return out\n\n# create CNN\nclass CNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(CNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.cnn = nn.Sequential(\n            nn.Conv1d(input_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Conv1d(hidden_size, hidden_size, 1),\n            nn.ReLU(),\n            nn.MaxPool1d(2),\n            nn.Flatten(),\n            nn.Linear(hidden_size*sequence_length, num_classes)\n        )\n\n    def forward(self, x):\n        out = self.cnn(x)"
    },
    {
        "prefix": "import torch ",
        "middle": "import sys ",
        "suffix": "import os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n",
        "generated_middle": "as nn\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):",
        "middle": "        super(RNN, self).__init__()",
        "suffix": "        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n",
        "generated_middle": "\n        super(RNN, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTM, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass GRU(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(GRU, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass LSTM_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(LSTM_2, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\nclass GRU_2(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(GRU_2, self).__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        retu"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)",
        "middle": "    def forward(self, x):",
        "suffix": "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out",
        "generated_middle": "\n        self.fc.weight.data.uniform_(-0.1, 0.1)\n        self.fc.bias.data.uniform_(-0.1, 0.1)\n\n    def forward(self, x):\n        x = x.view(-1, sequence_length, self.hidden_size)\n"
    },
    {
        "prefix": "import torch \nimport sys \nimport os\n\nimport torch.nn as nn \nfrom config import device",
        "middle": "# Hyperparameters ",
        "suffix": "sequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n",
        "generated_middle": "\n\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# create model\nclass RNN_model(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN_model, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\n# create model\nclass RNN_model_sequence(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN_model_sequence, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\n# create model\nclass RNN_model_sequence_sequence(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN_model_sequence_sequence, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\n# create model\nclass RNN_model_sequence_sequence_sequence(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN_model_sequence_sequence_sequence, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n\n# create model\nclass RNN_model_sequence_sequence_sequence_sequence(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN_model_sequence_sequence_sequence_sequence, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, "
    },
    {
        "prefix": "import torch \nimport sys ",
        "middle": "import os",
        "suffix": "\nimport torch.nn as nn \nfrom config import device\n# Hyperparameters \nsequence_length = 28 \n\n# create RNN\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n\n        # forward prop \n        out, _ = self.rnn(x, h0)\n        out = out.reshape(out.shape[0], -1)\n        out = self.fc(out)\n        \n        return out\n",
        "generated_middle": "as sys\nsys.path.append('../')\nfrom model import RNN\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n",
        "middle": "lang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')",
        "suffix": "lang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]",
        "middle": "def tokenize_lang_out(text):",
        "suffix": "    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)",
        "generated_middle": "\n\ndef tokenize_lang_out(text):\n"
    },
    {
        "prefix": "import spacy\n",
        "middle": "from torchtext.data import Field ",
        "suffix": "from torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "from torchtext.data import Field, TabularDataset\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to",
        "middle": "spacy_lang_in = spacy.load(from_)",
        "suffix": "spacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)",
        "generated_middle": "sp, to\n\n"
    },
    {
        "prefix": "import spacy\n",
        "middle": "from torchtext.data import Field ",
        "suffix": "from torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "from torchtext.data import Field, TabularDataset\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field ",
        "middle": "from torchtext.datasets import Multi30k",
        "suffix": "\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "as Field\nfrom torchtext.data import Multi30k\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n",
        "middle": "from config import  from_, to",
        "suffix": "\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "from torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import Bucket"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n",
        "middle": "lang_in.build_vocab(train_data, max_size=10000, min_freq=2)",
        "suffix": "",
        "generated_middle": "\ntrain_data.build_vocab(train_data.fields)\nvalid_data.build_vocab(valid_data.fields)\ntest_data.build_vocab(test_data.fields)\n\nprint(train_data.fields)\nprint(valid_data.fields)\nprint(test_data.fields)\n\nprint(train_data.vocab)\nprint(valid_data.vocab)\nprint(test_data.vocab)\n\nprint(train_data.vocab.stoi)\nprint(valid_data.vocab.stoi)\nprint(test_data.vocab.stoi)\n\nprint(train_data.vocab.itos)\nprint(valid_data.vocab.itos)\nprint(test_data.vocab.itos)\n\nprint(train_data.vocab.ngrams)\nprint(valid_data.vocab.ngrams)\nprint(test_data.vocab.ngrams)\n\nprint(train_data.vocab.ngrams.keys())\nprint(valid_data.vocab.ngrams.keys())\nprint(test_data.vocab.ngrams.keys())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())\nprint(valid_data.vocab.ngrams.items())\nprint(test_data.vocab.ngrams.items())\n\nprint(train_data.vocab.ngrams.values())\nprint(valid_data.vocab.ngrams.values())\nprint(test_data.vocab.ngrams.values())\n\nprint(train_data.vocab.ngrams.items())"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n",
        "middle": "spacy_lang_in = spacy.load(from_)",
        "suffix": "spacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "from_to import from_, to\n\nnlp = spacy.load(from_)\n"
    },
    {
        "prefix": "import spacy\n\nfrom torchtext.data import Field \nfrom torchtext.datasets import Multi30k\n\nfrom config import  from_, to\n\nspacy_lang_in = spacy.load(from_)\nspacy_lang_out = spacy.load(to)\n\ndef tokenize_lang_in(text):",
        "middle": "    return [tok.text for tok in spacy_lang_in.tokenizer(text)]",
        "suffix": "\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]\n\nlang_in = Field(tokenize=tokenize_lang_in, lower=True, init_token='<sos>', eos_token='<eos>')\nlang_out = Field(tokenize=tokenize_lang_out, lower=True, init_token='<sos>', eos_token='<eos>')\n\ntrain_data, valid_data, test_data = Multi30k.splits(\n    exts=(f'.{from_}', f'.{to}'), fields=(lang_in, lang_out), root='../text_data/.data'\n)\n\nlang_in.build_vocab(train_data, max_size=10000, min_freq=2)\nlang_out.build_vocab(train_data, max_size=10000, min_freq=2)\n",
        "generated_middle": "\n    return [tok.text for tok in spacy_lang_in.tokenizer(text)]\n\ndef tokenize_lang_out(text):\n    return [tok.text for tok in spacy_lang_out.tokenizer(text)]"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]",
        "middle": "        target_len = target.shape[0]",
        "suffix": "        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n        target_len = target.shape[0]\n\n        source_vocab_size = len(lang_in.vocab)\n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell",
        "middle": "class EncDec(nn.Module):",
        "suffix": "    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n\n\nclass EncoderDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):",
        "middle": "    def __init__(self, encoder, decoder):",
        "suffix": "        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n        def __init__(self, encoder, decoder):\n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n",
        "middle": "class EncDec(nn.Module):",
        "suffix": "    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "   \nclass EncoderDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n\n        return outputs, hidden, cell\n\nclass EncoderDecDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDecDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n\n        return outputs, hidden, cell\n\nclass EncoderDecDecDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDecDecDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n\n        return outputs, hidden, cell\n\nclass EncoderDecDecDecDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDecDecDecDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n\n        return outputs, hidden, cell\n\nclass EncoderDecDecDecDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDecDecDecDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zer"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)",
        "middle": "        self.hidden_size = hidden_size",
        "suffix": "        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return hidden, cell, outputs\n\nclass EncoderDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))",
        "middle": "        predictions = self.fc(outputs)",
        "suffix": "        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n\n        return outputs, hidden, cell\n    \nclass Attention(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Attention, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return outputs, hidden, cell\n    \nclass Attention_v2(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Attention_v2, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return outputs, hidden, cell\n    \nclass Attention_v3(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Attention_v3, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return outputs, hidden, cell\n    \nclass Attention_v4(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Attention_v4, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return outputs, hidden, cell\n    \nclass Attention_v5(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Attention_v5, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, ce"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)",
        "middle": "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)",
        "suffix": "\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return hidden, cell\n    \nclass Encoder_LSTM(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder_LSTM, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder_LSTM(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder_LSTM, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n\n        return hidden, cell\n    \nclass Encoder_GRU(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder_GRU, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder_GRU(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder_GRU, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.GRU(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self."
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'",
        "middle": "else:",
        "suffix": "    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\nelse:\n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell",
        "middle": "class Decoder(nn.Module):",
        "suffix": "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "\n\n    def init_hidden(self, batch_size):\n        return (torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device),\n                torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device))\n\n\nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\n\nclass EncoderDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncoderDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_in.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n\n        x = target[0]\n\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        \n"
    },
    {
        "prefix": "import torch\nimport random\n\nimport torch.nn as nn \nfrom data_loading import lang_in, lang_out\nfrom config import system\n\nif 'mac' in system.lower():\n    system = 'mps' \nelif 'win' in system.lower():\n    system = 'cpu'\nelse:\n    raise TypeError('Unknown system, type: \"Mac\" or \"Windows\"')\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else system)\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Encoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n\n    def forward(self, x):\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding)\n\n        return hidden, cell\n    \nclass Decoder(nn.Module):\n    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n        super(Decoder, self).__init__()\n        self.dropout = nn.Dropout(p)\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embedding = nn.Embedding(input_size, embedding_size)\n        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n        self.fc = nn.Linear(hidden_size, input_size)\n\n    def forward(self, x, hidden, cell):\n        x = x.unsqueeze(0)\n        embedding = self.dropout(self.embedding(x))\n        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n        predictions = self.fc(outputs)\n        predictions = predictions.squeeze(0)\n\n        return predictions, hidden, cell\n\nclass EncDec(nn.Module):\n    def __init__(self, encoder, decoder):\n        super(EncDec, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n\n    def forward(self, source, target, teacher_force_ratio=0.5):\n        batch_size = target.shape[1]\n        target_len = target.shape[0]\n        target_vocab_size = len(lang_out.vocab)\n\n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        hidden, cell = self.encoder(source)\n",
        "middle": "        x = target[0]",
        "suffix": "\n        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        ",
        "generated_middle": "        for t in range(1, target_len):\n            output, hidden, cell = self.decoder(x, hidden, cell)\n            outputs[t] = output\n            top1 = output.argmax(1)\n            x = target[t] if random.random() < teacher_force_ratio else top1\n        "
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n",
        "middle": "    # Set model to eval",
        "suffix": "    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "    for x, y in loader:\n        x = x.to(device=device).squeeze(1)\n        y = y.to(device=device)\n\n        scores = model(x)\n        _, predictions = scores.max(1)\n        num_correct += (predictions == y).sum()\n        num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n# test model on test set\ndef test_model(loader, model):\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0",
        "middle": "    # Set model to eval",
        "suffix": "    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n",
        "generated_middle": "\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n# Test model on test set\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.eval()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:2f}\")\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n\n# Test model on training set\ndef check_accuracy(loader, model):\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")",
        "middle": "print(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")",
        "suffix": "",
        "generated_middle": "\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:2f}\")"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step",
        "middle": "        optimizer.step()",
        "suffix": "\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "e step \n        optimizer.step()"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n",
        "middle": "        # gradient descent or adam step",
        "suffix": "        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "        optimizer.step()\n"
    },
    {
        "prefix": "import torch ",
        "middle": "import torch.nn as nn",
        "suffix": "import torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "as nn\n"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):",
        "middle": "    num_correct = 0",
        "suffix": "    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "\n    correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\n\ndef check_accuracy_test(loader, model):\n    correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy_test(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy_test(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\n\ndef check_accuracy_train(loader, model):\n    correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy_train(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy_train(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\n\ndef check_accuracy_train_test(loader, model):\n    correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy_train_test(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy_train_test(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\n\ndef check_accuracy_train_test_test(loader, model):\n    correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint("
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)",
        "middle": "            y = y.to(device=device)",
        "suffix": "\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "\n            y = y.to(device=device)\n\n            # forward \n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\ndef check_accuracy_test(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            # forward \n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy_test(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy_test(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\ndef check_accuracy_train(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval",
        "middle": "    model.eval()",
        "suffix": "\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "ing mode\n    model.eval()\n\n    # Loop through data\n    for data, targets in loader:\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        _, predictions = scores.max(1)\n        num_correct += (predictions == targets).sum()\n        num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n# Test model on test set\ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to evaling mode\n    model.eval()\n\n    # Loop through data\n    for data, targets in loader:\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        _, predictions = scores.max(1)\n        num_correct += (predictions == targets).sum()\n        num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:2f}\")\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:.2f}\")\n\n\n# Train model on training set\ndef check_accuracy(loader, model):"
    },
    {
        "prefix": "import torch \nimport torch.nn as nn\nimport torch.optim as optim\nfrom Net import RNN\nfrom config import device\nfrom load_dataset import test_loader, train_loader\n\n# Hyperparameters \ninput_size = 28\nsequence_length = 28 \nnum_layers = 2\nhidden_size = 256 \nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 2\n\n# initialize network\nmodel = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n\n# Loss and optimizer \ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Train network \nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # get data to cuda if possible\n        data = data.to(device).squeeze(1)\n        targets = targets.to(device)\n\n        # forward \n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward \n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()\n\n\n# check accuracy on training and test to see how good our model\n        \ndef check_accuracy(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)",
        "middle": "            y = y.to(device=device)",
        "suffix": "\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n",
        "generated_middle": "\n            y = y.to(device=device)\n\n            # forward \n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\ndef check_accuracy_test(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device).squeeze(1)\n            y = y.to(device=device)\n\n            # forward \n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n    # Toggle model back to train\n    model.train()\n    return num_correct / num_samples\n\n\nprint(f\"Accuracy on training set: {check_accuracy_test(train_loader, model)*100:2f}\")\nprint(f\"Accuracy on test set: {check_accuracy_test(test_loader, model)*100:.2f}\")\n\n\n# check accuracy on training and test to see how good our model\ndef check_accuracy_train(loader, model):\n    num_correct = 0\n    num_samples = 0\n\n    # Set model to eval\n    model.eval()\n\n    with torch.no_grad():"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}",
        "middle": "    save_checkpoint(checkpoint)",
        "suffix": "\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n    # Save checkpoint\n    save_checkpoint(checkpoint, model, optimizer)\n\n    # Evaluate model on test data\n    score = bleu(test_data[1:100], model, lang_in, lang_out, device)\n    print(f\"Bleu score {score*100:.2f}\")\n"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop",
        "middle": "        loss.backward()",
        "suffix": "\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "ualt step\n        loss.backward()"
    },
    {
        "prefix": "import torch",
        "middle": "import torch.optim as optim",
        "suffix": "import torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": ".nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nfrom torchtext.data import BucketIterator\nf"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)",
        "middle": "    model.eval()",
        "suffix": "\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n    score = bleu(test_data[1:100], model, lang_in, lang_out, device)\n    print(f\"Bleu score {score*100:.2f}\")\n\n    # Save model"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)",
        "middle": "encoder_embedding_size = 300",
        "suffix": "decoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "\n"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n",
        "middle": "pad_idx = lang_out.vocab.stoi['<pad>']",
        "suffix": "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "model.train()\n\nfor batch_idx, batch in enumerate(train_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Gradient descent step\n    optimizer.step()\n\n    # Plot to tensorboard\n    writer.add_scalar(\"Training loss\", loss, global_step=step)\n    step += 1\n\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(valid_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Gradient descent step\n    optimizer.step()\n\n    # Plot to tensorboard\n    writer.add_scalar(\"Validation loss\", loss, global_step=step)\n    step += 1\n\n\nmodel.train()\n\nfor batch_idx, batch in enumerate(test_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Gradient descent step\n    optimizer.step()\n\n    # Plot to tensorboard\n    writer.add_scalar(\"Test loss\", loss, global_step=step)\n    step += 1\n\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(test_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)",
        "middle": "pad_idx = lang_out.vocab.stoi['<pad>']",
        "suffix": "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "\n\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(valid_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Validation loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\nscore = bleu(test_data[1:100], m"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n",
        "middle": "    print(f\"Translated example sentence: \\n {translated_sentence}\")",
        "suffix": "\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "    print(f\"Translated sentence: {translated_sentence}\")\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Validation\n    valid_loss, valid_acc = valid_model(valid_iterator, model, criterion)\n    print(f\"Validation loss: {valid_loss:.4f} | Validation accuracy: {valid_acc*100:.2f}\")\n\n    # Test\n    test_loss, test_acc = test_model(test_iterator, model, criterion)\n    print(f\"Test loss: {test_loss:.4f} | Test accuracy: {test_acc*100:.2f}\")\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # Save the model checkpoint\n    save_checkpoint(checkpoint, epoch)\n\n    # S"
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,\n    encoder_embedding_size,\n    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(",
        "middle": "    input_size_decoder,",
        "suffix": "    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": "input_size_decoder, decoder_embedding_size, hidden_size, output_size).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\n# Test\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(valid_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Gradient descent step\n    optimizer.step()\n\n    # Plot to tensorboard\n    writer.add_scalar(\"Validation loss\", loss, global_step=step)\n    step += 1\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\n# Test\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(test_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if "
    },
    {
        "prefix": "import torch\n\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchtext.data import BucketIterator\n\nfrom func import translate_sentence, bleu, save_checkpoint, load_checkpoint\nfrom data_loading import lang_in, lang_out, train_data, valid_data, test_data\nfrom Net import Encoder, Decoder, EncDec, device\n\n# Training hyperparameters\nnum_epochs = 100\nlearning_rate = 0.001\nbatch_size = 64\n\n# Model hyperparameters\nload_model = False\ninput_size_encoder = len(lang_in.vocab)\ninput_size_decoder = len(lang_out.vocab)\noutput_size = len(lang_out.vocab)\nencoder_embedding_size = 300\ndecoder_embedding_size = 300\nhidden_size = 1024  # Needs to be the same for both RNN's\nnum_layers = 2\nenc_dropout = 0.5\ndec_dropout = 0.5\n\n# Tensorboard to get nice loss plot\nwriter = SummaryWriter(f\"runs/loss_plot\")\nstep = 0\n\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size=batch_size,\n    device=device,\n    sort_within_batch=True,\n    sort_key=lambda x: len(x.src),\n    repeat=False\n)\n\nencoder_net = Encoder(\n    input_size_encoder,",
        "middle": "    encoder_embedding_size,",
        "suffix": "    hidden_size,\n    num_layers,\n    dec_dropout,\n    output_size\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")",
        "generated_middle": " hidden_size, input_size_decoder, output_size, enc_dropout, dec_dropout\n).to(device)\n\ndecoder_net = Decoder(\n    input_size_decoder,\n    decoder_embedding_size,\n    hidden_size,\n    output_size,\n    dec_dropout\n).to(device)\n\nmodel = EncDec(encoder_net, decoder_net).to(device)\noptimizer = optim.Adam(model.parameters, lr=learning_rate)\n\npad_idx = lang_out.vocab.stoi['<pad>']\ncriterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n\nif load_model:\n    load_checkpoint(torch.load(\"model.pth\"), model, optimizer)\n\nsentence = \"ein boot mit mehreren m\u00e4nnern darauf wird von einem gro\u00dfen pferdegespann ans ufer gezogen.\"\n\nfor epoch in range(num_epochs):\n    print(f\"[Epoch {epoch} / {num_epochs}]\")\n\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    save_checkpoint(checkpoint)\n\n    model.eval()\n\n    translated_sentence = translate_sentence(\n        model, sentence, lang_in, lang_out, device, max_length=50\n    )\n\n    print(f\"Translated example sentence: \\n {translated_sentence}\")\n\n    model.train()\n\n    for batch_idx, batch in enumerate(train_iterator):\n        inp_data = batch.src.to(device)\n        target = batch.trg.to(device)\n\n        # Forward prop\n        output = model(inp_data, target)\n\n        # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n        # doesn't take input in that form. For example if we have MNIST we want to have\n        # output to be: (N, 10) and targets just (N).\n        output = output[1:].reshape(-1, output.shape[2])\n        target = target[1:].reshape(-1)\n\n        optimizer.zero_grad()\n        loss = criterion(output, target)\n\n        # Back prop\n        loss.backward()\n\n        # Clip to avoid exploding gradient issues, makes sure grads are\n        # within a healthy range\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n        # Gradient descent step\n        optimizer.step()\n\n        # Plot to tensorboard\n        writer.add_scalar(\"Training loss\", loss, global_step=step)\n        step += 1\n\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\n# Test\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(valid_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = model(inp_data, target)\n\n    # Output is of shape (trg_len, batch_size, output_dim) but Cross Entropy Loss\n    # doesn't take input in that form. For example if we have MNIST we want to have\n    # output to be: (N, 10) and targets just (N).\n    output = output[1:].reshape(-1, output.shape[2])\n    target = target[1:].reshape(-1)\n\n    optimizer.zero_grad()\n    loss = criterion(output, target)\n\n    # Back prop\n    loss.backward()\n\n    # Clip to avoid exploding gradient issues, makes sure grads are\n    # within a healthy range\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n\n    # Gradient descent step\n    optimizer.step()\n\n    # Plot to tensorboard\n    writer.add_scalar(\"Validation loss\", loss, global_step=step)\n    step += 1\n\nscore = bleu(test_data[1:100], model, lang_in, lang_out, device)\nprint(f\"Bleu score {score*100:.2f}\")\n\n# Test\n\nmodel.eval()\n\nfor batch_idx, batch in enumerate(test_iterator):\n    inp_data = batch.src.to(device)\n    target = batch.trg.to(device)\n\n    # Forward prop\n    output = m"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n",
        "middle": "        return text_input.long(), text_target.long()",
        "suffix": "    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n        return text_input, text_target\n\n    def get_random_batch_with_char(self, char_str):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(char_str[:-1])\n            text_target[i, :] = self.char_tensor(char_str[1:])\n"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(",
        "middle": "                initial_input[p].view(1).to(device), hidden, cell",
        "suffix": "            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "initial_input.view(1).to(device), hidden, cell)\n            output, (hidden, cell) = self.rnn(initial_input.view(1).to(device), hidden, cell)\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            initial_input = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\nclass Discriminator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):",
        "middle": "        tensor = torch.zeros(len(string)).long()",
        "suffix": "        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_characters, self.chunk_len)\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def char_tensor_batch_batch_batch(self, string):\n        tensor = torch.zeros(self.n_character"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure",
        "middle": "            self.rnn.load_state_dict(checkpoint['model_state_dict'])",
        "suffix": "            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def test(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting testing\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Testing loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def generate_from_file(self, file_path):\n        with open(file_path, 'r') as f:\n            text = f.read()\n\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text[:-1])\n            text_target[i, :] = self.char_tensor(text[1:])\n\n        return text_input.long(), text_target.long()\n\n    def generate_from_file_with_char(self, file_path, char_list):\n        with open(file_path, 'r') as f:\n            text = f.read()\n\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text[:-1])\n            text_target[i, :] = self.char_tensor(text[1:])\n\n        return text_input.long(), text_target.long()\n\n    def generate_from_file_with_char_with_len(self, file_path, char_list, len_list):\n        with open(file_path, 'r') as f:\n            text = f.read()\n\n        text_"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)",
        "middle": "            criterion = nn.CrossEntropyLoss()",
        "suffix": "            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n            criterion = nn.CrossEntropyLoss()\n\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # define the criterion\n            criterion = nn.CrossEntropyLoss()\n\n            # define the training loop\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        else:\n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def test(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # define the criterion\n            criterion = nn.CrossEntropyLoss()\n\n            # define the training loop\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # define the criterion\n            criterion = nn.CrossEntropyLoss()\n\n            # define the training loop\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n         "
    },
    {
        "prefix": "import torch \nimport random\nimport string",
        "middle": "import torch.nn as nn ",
        "suffix": "from config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\nimport string\n"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)",
        "middle": "                    loss += criterion(output, target[:, c])",
        "suffix": "\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n                    output_dist = output.data.view(-1).div(temperature).exp()\n                    top_char = torch.multinomial(output_dist, 1)[0]\n                    predicted_char = self.all_characters[top_char]\n                    predicted += predicted_char\n                    last_char = self.char_tensor(predicted_char)\n\n                    loss += criterion(output, target) * self.chunk_len\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        else:\n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def test(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting testing\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    output_dist = output.data.view(-1).div(temperature).exp()\n                    top_char = torch.multinomial(output_dist, 1)[0]\n                    predicted_char = self.all_characters[top_char]\n                    predicted += predicted_char\n                    last_char = self.char_tensor(predicted_char)\n\n                    loss += criterion(output, target) * self.chunk_len\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Testing loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def generate(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n    "
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n",
        "middle": "        last_char = initial_input[-1]",
        "suffix": "\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "            predicted += self.all_characters[p]\n            initial_input = self.char_tensor(initial_str)\n\n        return predicted, initial_input\n    \n    def rnn(self, text_input, hidden, cell):\n        last_char = self.char_tensor(text_input)"
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer",
        "middle": "            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)",
        "suffix": "\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "s\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names1\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def test(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names2\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizers\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names3\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n              "
    },
    {
        "prefix": "import torch \nimport random\nimport string\nimport torch.nn as nn \nfrom config import device, file, first_run\n\nfrom torch.utils.tensorboard import SummaryWriter\n\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, out_size):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n\n        self.embed = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, out_size)\n\n    def forward(self, x, hidden, cell):\n        out = self.embed(x)\n        out, (hidden, cell) = self.lstm(out.unsqueeze(1), (hidden, cell))\n        out = self.fc(out.reshape(out.shape[1], -1))\n        return out, (hidden, cell)\n    \n    def init_hidden(self, batch_size):\n        hidden = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        cell = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n        return hidden, cell\n    \nclass Generator:\n    def __init__(self, \n                chunk_len = 250,\n                num_epochs = 5000,\n                batch_size = 1,\n                print_every = 50,\n                hidden_size = 256,\n                num_layers = 2,\n                lr = 0.003):\n        \n        self.chunk_len = chunk_len\n        self.num_epochs = num_epochs\n        self.batch_size = batch_size\n        self.print_every = print_every\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.lr = lr\n        self.all_characters = string.printable\n        self.n_characters = len(self.all_characters)\n\n\n    def char_tensor(self, string):\n        tensor = torch.zeros(len(string)).long()\n        for c in range(len(string)):\n            tensor[c] = self.all_characters.index(string[c])\n\n        return tensor\n    \n    def get_random_batch(self):\n        start_idx = random.randint(0, len(file) - self.chunk_len)\n        end_idx = start_idx + self.chunk_len + 1\n        text_str = file[start_idx: end_idx]\n        text_input = torch.zeros(self.batch_size, self.chunk_len)\n        text_target = torch.zeros(self.batch_size, self.chunk_len)\n\n        for i in range(self.batch_size):\n            text_input[i, :] = self.char_tensor(text_str[:-1])\n            text_target[i, :] = self.char_tensor(text_str[1:])\n\n        return text_input.long(), text_target.long()\n    \n    def generate(self, initial_str = 'A', predict_len = 200, temperature = 0.85):\n\n        initial_str = initial_str.lower()\n\n        hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n        initial_input = self.char_tensor(initial_str)\n        predicted = initial_str\n\n        for p in range(len(initial_str)-1):\n            _, (hidden, cell) = self.rnn(\n                initial_input[p].view(1).to(device), hidden, cell\n            )\n\n        last_char = initial_input[-1]\n\n        for p in range(predict_len):\n            output, (hidden, cell) = self.rnn(\n                last_char.view(1).to(device), hidden, cell\n            )\n            output_dist = output.data.view(-1).div(temperature).exp()\n            top_char = torch.multinomial(output_dist, 1)[0]\n            predicted_char = self.all_characters[top_char]\n            predicted += predicted_char\n            last_char = self.char_tensor(predicted_char)\n\n        return predicted\n    \n    \n    def train(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting training\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):",
        "middle": "                    output, (hidden, cell) = self.rnn(inp[:, c], hidden, cell)",
        "suffix": "                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")",
        "generated_middle": "\n                    inp, target = self.get_random_batch()\n                    output, (hidden, cell) = self.rnn(\n                        inp.view(1).to(device), hidden, cell\n                    )\n                    output_dist = output.data.view(-1).div(temperature).exp()\n                    top_char = torch.multinomial(output_dist, 1)[0]\n                    predicted_char = self.all_characters[top_char]\n                    predicted += predicted_char\n                    last_char = self.char_tensor(predicted_char)\n\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Training loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        else:\n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def test(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_characters\n            ).to(device)\n\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n            criterion = nn.CrossEntropyLoss()\n            writer = SummaryWriter(f\"runs/names0\")  # for tensorboard\n\n            print(\"=> Starting testing\")\n\n            for epoch in range(1, self.num_epochs + 1):\n                inp, target = self.get_random_batch()\n                hidden, cell = self.rnn.init_hidden(batch_size=self.batch_size)\n\n                self.rnn.zero_grad()\n                loss = 0\n                inp = inp.to(device)\n                target = target.to(device)\n\n                for c in range(self.chunk_len):\n                    inp, target = self.get_random_batch()\n                    output, (hidden, cell) = self.rnn(\n                        inp.view(1).to(device), hidden, cell\n                    )\n                    output_dist = output.data.view(-1).div(temperature).exp()\n                    top_char = torch.multinomial(output_dist, 1)[0]\n                    predicted_char = self.all_characters[top_char]\n                    predicted += predicted_char\n                    last_char = self.char_tensor(predicted_char)\n\n                    loss += criterion(output, target[:, c])\n\n                loss.backward()\n                optimizer.step()\n                loss = loss.item() / self.chunk_len\n\n                if epoch % self.print_every == 0:\n                    print(f\"Loss: {loss}, Epoch: {epoch}\")\n                    print(self.generate())\n\n                writer.add_scalar(\"Testing loss\", loss, global_step=epoch)\n\n            # save the model as a state dictionary in pth format\n            torch.save({\n                'model_state_dict': self.rnn.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n            },  'model_weights/model.pth')\n\n        elif not first_run:\n            # define the structure and the parameters\n            self.rnn = RNN(self.n_characters, \n                            self.hidden_size, \n                            self.num_layers,\n                            self.n_characters)\n            \n            # define the optimizer\n            optimizer = torch.optim.Adam(self.rnn.parameters(), lr=self.lr)\n\n            # load the checkpoint data\n            checkpoint = torch.load('model_weights/model.pth')\n            # load the checkpoint into the model structure\n            self.rnn.load_state_dict(checkpoint['model_state_dict'])\n            # load the checkpoint to the optimizer structure\n            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n            # pass model to the device\n            self.rnn.to(device)\n\n        else: \n            raise TypeError(f\"Input value must be boolean instead of {first_run} in config.py\")\n\n    def generate(self):\n        if first_run:\n            self.rnn = RNN(\n                self.n_characters, \n                self.hidden_size, \n                self.num_layers,\n                self.n_character"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)",
        "middle": "    # sys.exit()",
        "suffix": "\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n    # print(german.vocab.itos)\n    # print(english.vocab.itos)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_german(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n    # print(german.vocab.itos)\n    # print(english.vocab.itos)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_english(model, sentence, english, device, max_length=50):\n    # print(sentence)\n    # print(english.vocab.itos)\n    # print(german.vocab.itos)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(token"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n",
        "middle": "        outputs.append(best_guess)",
        "suffix": "\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "            outputs.append(best_guess)\n\n    return outputs\n\n\ndef translate_sentence_batch(model, sentences, german, english, device, max_length=50):\n    # print(sentences)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentences) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentences)]\n    else:\n        tokens = [token.lower() for token in sentences]\n\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n            outputs.append(best_guess)\n\n    return outputs\n\n\ndef translate_sentence_batch_batch(model, sentences, german, english, device, max_length=50):\n    # print(sentences)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentences) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentences)]\n    else:\n        tokens = [token.lower() for token in sentences]\n\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n            outputs.append(best_guess)\n\n    return outputs\n\n\ndef translate_sentence_batch_batch_batch(model, sentences, german, english, device, max_length=50):\n    # print(sentences)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentences) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentences)]\n    else:\n        tokens = [token.lower() for token in sentences]\n\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()",
        "middle": "        outputs.append(best_guess)",
        "suffix": "\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n\n        outputs.append(best_guess)"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()",
        "middle": "        outputs.append(best_guess)",
        "suffix": "\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n\n        outputs.append(best_guess)"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):",
        "middle": "    print(\"=> Loading checkpoint\")",
        "suffix": "    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n    print(\"=> Loading checkpoint\")\n    checkpoint = torch.load(checkpoint)\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n\ndef train(model, data, optimizer, device, max_length=50):\n    # print(data)\n    # print(model)\n    # print(optimizer)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(data) == str:\n        tokens = [token.text.lower() for token in spacy_ger(data)]\n    else:\n        tokens = [token.lower() for token in data]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef evaluate(model, data, device, max_length=50):\n    # print(data)\n    # print(model)\n    # print(optimizer)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(data) == str:\n        tokens = [token.text.lower() for token in spacy_ger(data)]\n    else:\n        tokens = [token.lower() for token in data]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef predict(model, data, device, max_length=50):\n    # print(data)\n    # print(model)\n    # print(optimizer)\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(data) == str:\n        tokens = [token.text.lower() for token "
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n",
        "middle": "    for example in data:",
        "suffix": "        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef bleu_score(data, model, german, english, device):\n    targets = []\n    outputs = []\n    for example in data:\n"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n",
        "middle": "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):",
        "suffix": "    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n\nif __name__ == \"__main__\":\n    # Load data\n    data = []\n    with open(\"data/train.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkpoint = torch.load(filename)\n    model.load_state_dict(checkpoint[\"model_state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n\n    # Load data\n    data = []\n    with open(\"data/test.txt\", \"r\") as f:\n        for line in f:\n            data.append(line.strip())\n\n    # Load model\n    model = spacy.load(\"de\")\n\n    # Load optimizer\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n    # Load checkpoint\n    filename = \"checkpoint.pth\"\n    checkp"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):",
        "middle": "    print(\"=> Saving checkpoint\")",
        "suffix": "    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:",
        "middle": "        tokens = [token.lower() for token in sentence]",
        "suffix": "\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n        tokens = [token.text for token in sentence]\n\n    # Remove <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_german(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.text for token in sentence]\n\n    # Remove <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_english(model, sentence, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load english tokenizer\n    spacy_english = spacy.load(\"en\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    if type(sentence) == str:\n        tokens = [token.text.lower() for token in spacy_english(sentence)]\n    else:\n        tokens = [token.text for token in sentence]\n\n    # Remove <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, english.init_token)\n    tokens.append(english.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [english.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n    "
    },
    {
        "prefix": "import torch\nimport spacy\nfrom torchtext.data.metrics import bleu_score\n\n\ndef translate_sentence(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)",
        "middle": "    if type(sentence) == str:",
        "suffix": "        tokens = [token.text.lower() for token in spacy_ger(sentence)]\n    else:\n        tokens = [token.lower() for token in sentence]\n\n    # print(tokens)\n\n    # sys.exit()\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef bleu(data, model, german, english, device):\n    targets = []\n    outputs = []\n\n    for example in data:\n        src = vars(example)[\"src\"]\n        trg = vars(example)[\"trg\"]\n\n        prediction = translate_sentence(model, src, german, english, device)\n        prediction = prediction[:-1]  # remove <eos> token\n\n        targets.append([trg])\n        outputs.append(prediction)\n\n    return bleu_score(outputs, targets)\n\n\ndef save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n    print(\"=> Saving checkpoint\")\n    torch.save(state, filename)\n\n\ndef load_checkpoint(checkpoint, model, optimizer):\n    print(\"=> Loading checkpoint\")\n    model.load_state_dict(checkpoint[\"state_dict\"])\n    optimizer.load_state_dict(checkpoint[\"optimizer\"])",
        "generated_middle": "\n    tokens = [token.text.lower() for token in spacy_ger(sentence)]\n\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_german(model, sentence, german, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n    tokens = [token.text.lower() for token in spacy_ger(sentence)]\n\n    # Add <SOS> and <EOS> in beginning and end respectively\n    tokens.insert(0, german.init_token)\n    tokens.append(german.eos_token)\n\n    # Go through each german token and convert to an index\n    text_to_indices = [german.vocab.stoi[token] for token in tokens]\n\n    # Convert to Tensor\n    sentence_tensor = torch.LongTensor(text_to_indices).unsqueeze(1).to(device)\n\n    # Build encoder hidden, cell state\n    with torch.no_grad():\n        hidden, cell = model.encoder(sentence_tensor)\n\n    outputs = [english.vocab.stoi[\"<sos>\"]]\n\n    for _ in range(max_length):\n        previous_word = torch.LongTensor([outputs[-1]]).to(device)\n\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(previous_word, hidden, cell)\n            best_guess = output.argmax(1).item()\n\n        outputs.append(best_guess)\n\n        # Model predicts it's the end of the sentence\n        if output.argmax(1).item() == english.vocab.stoi[\"<eos>\"]:\n            break\n\n    translated_sentence = [english.vocab.itos[idx] for idx in outputs]\n\n    # remove start token\n    return translated_sentence[1:]\n\n\ndef translate_sentence_english(model, sentence, english, device, max_length=50):\n    # print(sentence)\n\n    # sys.exit()\n\n    # Load german tokenizer\n    spacy_ger = spacy.load(\"de\")\n\n    # Create tokens using spacy and everything in lower case (which is what our vocab is)\n"
    }
]