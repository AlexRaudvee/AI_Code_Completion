{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "\n",
    "\n",
    "# FUNCTIONS FOR GENERAL USE\n",
    "\n",
    "def load_examples_from_json(file_path):\n",
    "    \"\"\"\n",
    "    Loads examples from a JSON file and returns them as a list of dictionaries.\n",
    "    \n",
    "    Parameters:\n",
    "    - file_path (str): Path to the JSON file containing examples.\n",
    "    \n",
    "    Returns:\n",
    "    - examples (list): A list of dictionaries, each with 'prefix', 'middle', and 'suffix'.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as f:\n",
    "        examples = json.load(f)\n",
    "    return examples\n",
    "\n",
    "\n",
    "# FUNCTIONS FOR DATASET CREATION\n",
    "\n",
    "def load_code_files(directory_path, file_extension=\".py\"):\n",
    "    \"\"\"\n",
    "    Loads code files from a specified directory and returns their content along with the count of files.\n",
    "\n",
    "    Parameters:\n",
    "    ---\n",
    "    - directory_path (str): Path to the directory containing code files.\n",
    "    - file_extension (str): File extension to filter code files, default is \".py\".\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    - tuple: A tuple containing:\n",
    "        - code_snippets (list of str): List of code content strings from each file.\n",
    "        - files_num (int): Number of files processed and loaded.\n",
    "\n",
    "    Example:\n",
    "    ---\n",
    "    >>> code_snippets, files_num = load_code_files(\"path/to/directory\")\n",
    "    >>> print(files_num)\n",
    "    3  # Assuming there were 3 files with .py extension\n",
    "    \"\"\"\n",
    "    code_snippets = []\n",
    "    files_num = 0\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(file_extension):\n",
    "            with open(os.path.join(directory_path, filename), \"r\") as f:\n",
    "                code_snippets.append(f.read())\n",
    "            files_num += 1\n",
    "    return code_snippets, files_num\n",
    "\n",
    "def adjust_empty_sections(prefix, middle, suffix):\n",
    "    \"\"\"\n",
    "    Adjusts the prefix, middle, and suffix sections of code to ensure none are empty.\n",
    "    \n",
    "    If `middle` is empty, it moves one line from `suffix` to `middle` and removes this line from `suffix`.\n",
    "    If `suffix` is empty, it shifts one line up from `prefix` to `middle` and from `middle` to `suffix`,\n",
    "    ensuring all three sections are populated if possible.\n",
    "\n",
    "    Parameters:\n",
    "    ---\n",
    "    - prefix (str): The prefix section of code, representing content before the cursor.\n",
    "    - middle (str): The middle section, representing code where a user might expect completion.\n",
    "    - suffix (str): The suffix section of code, representing content after the cursor.\n",
    "\n",
    "    Returns:\n",
    "    ---\n",
    "    - tuple: A tuple containing the adjusted `prefix`, `middle`, and `suffix` strings.\n",
    "\n",
    "    Example:\n",
    "    ---\n",
    "    >>> prefix, middle, suffix = adjust_empty_sections(\"line1\\nline2\", \"\", \"line3\\nline4\")\n",
    "    >>> print(middle)\n",
    "    \"line3\"  # The first line from suffix moves to middle\n",
    "    \"\"\"\n",
    "    # Split prefix, middle, and suffix into lines for easier manipulation\n",
    "    prefix_lines = prefix.splitlines()\n",
    "    suffix_lines = suffix.splitlines()\n",
    "\n",
    "    # If middle is empty, add one line from suffix and remove it from suffix\n",
    "    if not middle.strip() and suffix_lines:\n",
    "        middle = suffix_lines.pop(0)\n",
    "        suffix = \"\\n\".join(suffix_lines)\n",
    "    \n",
    "    # If suffix is empty, shift lines up to fill suffix and middle from prefix\n",
    "    if not suffix.strip():\n",
    "        # If possible, add the last line of prefix to middle\n",
    "        if prefix_lines:\n",
    "            middle = prefix_lines.pop()\n",
    "            prefix = \"\\n\".join(prefix_lines)\n",
    "    \n",
    "    return prefix, middle, suffix\n",
    "\n",
    "\n",
    "# FUNCTIONS FOR MODEL APPLICATION \n",
    "\n",
    "import torch\n",
    "\n",
    "def generate_completion(model, tokenizer, device, prefix, suffix, max_new_tokens=2000):\n",
    "    \"\"\"Generate code completion for the missing middle part given prefix and suffix.\"\"\"\n",
    "    \n",
    "    # Format the input using the <fim_prefix>, <fim_suffix>, and <fim_middle> tokens\n",
    "    input_text = f\"<fim_prefix>{prefix}<fim_suffix>{suffix}<fim_middle>\"\n",
    "    \n",
    "    # Encode the input and move it to the device (if required by model setup)\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    \n",
    "    # Generate the completion without specifying max_length, but limiting new tokens\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=max_new_tokens,  # Limit only the generated tokens\n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the output and return only the generated middle part\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "    print(generated_text)\n",
    "    print(type(generated_text))\n",
    "    # Remove prefix and suffix to get only the generated middle part\n",
    "    start_index = generated_text.find(prefix) + len(prefix)\n",
    "    end_index = generated_text.find(suffix, start_index)\n",
    "    \n",
    "    middle_text = generated_text[generated_text.find(\"<fim_middle>\") + len(\"<fim_middle>\"): len(generated_text) - len(\"<|endoftext|>\")]\n",
    "    return middle_text\n",
    "\n",
    "\n",
    "# FUNCTIONS FOR EVALUATIONS\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.chrf_score import sentence_chrf\n",
    "\n",
    "\n",
    "def exact_match(predicted, actual):\n",
    "    \"\"\"Check if the predicted text matches the actual text exactly.\"\"\"\n",
    "    return int(predicted.strip() == actual.strip())\n",
    "\n",
    "def compute_bleu(predicted, actual):\n",
    "    \"\"\"Compute BLEU score.\"\"\"\n",
    "    weights = [\n",
    "        (1.),\n",
    "        (1./2., 1./2.),\n",
    "        (1./3., 1./3., 1./3.),\n",
    "        (1./4., 1./4., 1./4., 1./4.)]\n",
    "    return sentence_bleu([actual.split()], predicted.split(), weights=weights)\n",
    "\n",
    "def compute_chrf(predicted, actual):\n",
    "    \"\"\"Compute ChrF score.\"\"\"\n",
    "    return sentence_chrf([actual], predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prefix': 'import torch \\nimport sys \\nimport os\\n\\nimport torch.nn as nn \\nfrom config import device\\n# Hyperparameters \\nsequence_length = 28 \\n\\n# create RNN\\nclass RNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(RNN, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):', 'middle': '        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)', 'suffix': '\\n        # forward prop \\n        out, _ = self.rnn(x, h0)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n', 'generated_middle': '<fim_suffix>'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      " 50%|█████     | 1/2 [01:36<01:36, 96.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>import torch \n",
      "import sys \n",
      "import os\n",
      "\n",
      "import torch.nn as nn \n",
      "from config import device\n",
      "# Hyperparameters \n",
      "sequence_length = 28 \n",
      "\n",
      "# create RNN\n",
      "class RNN(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(RNN, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):<fim_suffix>\n",
      "        # forward prop \n",
      "        out, _ = self.rnn(x, h0)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "<fim_middle>\n",
      "        # forward forward prop \n",
      "        out, _ = self.rnn(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create CNN\n",
      "class CNN(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(CNN, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.cnn = nn.Sequential(\n",
      "            nn.Conv2d(input_size, hidden_size, kernel_size=3, padding=1),\n",
      "            nn.ReLU(),\n",
      "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
      "            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\n",
      "            nn.ReLU(),\n",
      "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
      "            nn.Flatten(),\n",
      "            nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "        )\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.cnn(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create LSTM\n",
      "class LSTM(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(LSTM, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.lstm(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create GRU\n",
      "class GRU(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(GRU, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.gru(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create LSTM\n",
      "class LSTM_2(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(LSTM_2, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.lstm(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create GRU\n",
      "class GRU_2(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(GRU_2, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.gru(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create LSTM\n",
      "class LSTM_3(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(LSTM_3, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.lstm(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create GRU\n",
      "class GRU_3(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(GRU_3, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.gru(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create LSTM\n",
      "class LSTM_4(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(LSTM_4, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.lstm(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create GRU\n",
      "class GRU_4(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(GRU_4, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.gru(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create LSTM\n",
      "class LSTM_5(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(LSTM_5, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.lstm(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "\n",
      "# create GRU\n",
      "class GRU_5(nn.Module):\n",
      "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(GRU_5, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        # forward forward prop \n",
      "        out, _ = self.gru(x)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "       \n",
      "<class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [01:37<00:00, 48.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<fim_prefix>import torch \n",
      "import sys \n",
      "import os\n",
      "\n",
      "import torch.nn as nn \n",
      "from config import device\n",
      "# Hyperparameters \n",
      "sequence_length = 28 \n",
      "\n",
      "# create RNN<fim_suffix>    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
      "        super(RNN, self).__init__()\n",
      "        self.hidden_size = hidden_size\n",
      "        self.num_layers = num_layers\n",
      "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
      "        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\n",
      "\n",
      "    def forward(self, x):\n",
      "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
      "\n",
      "        # forward prop \n",
      "        out, _ = self.rnn(x, h0)\n",
      "        out = out.reshape(out.shape[0], -1)\n",
      "        out = self.fc(out)\n",
      "        \n",
      "        return out\n",
      "<fim_middle>\n",
      "class RNN(nn.Module):\n",
      "<|endoftext|>\n",
      "<class 'str'>\n",
      "{'prefix': 'import torch \\nimport sys \\nimport os\\n\\nimport torch.nn as nn \\nfrom config import device\\n# Hyperparameters \\nsequence_length = 28 \\n\\n# create RNN\\nclass RNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(RNN, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):', 'middle': '        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)', 'suffix': '\\n        # forward prop \\n        out, _ = self.rnn(x, h0)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n', 'generated_middle': '\\n        # forward forward prop \\n        out, _ = self.rnn(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create CNN\\nclass CNN(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(CNN, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.cnn = nn.Sequential(\\n            nn.Conv2d(input_size, hidden_size, kernel_size=3, padding=1),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2, stride=2),\\n            nn.Conv2d(hidden_size, hidden_size, kernel_size=3, padding=1),\\n            nn.ReLU(),\\n            nn.MaxPool2d(kernel_size=2, stride=2),\\n            nn.Flatten(),\\n            nn.Linear(hidden_size*sequence_length, num_classes)\\n        )\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.cnn(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create LSTM\\nclass LSTM(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(LSTM, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.lstm(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create GRU\\nclass GRU(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(GRU, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.gru(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create LSTM\\nclass LSTM_2(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(LSTM_2, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.lstm(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create GRU\\nclass GRU_2(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(GRU_2, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.gru(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create LSTM\\nclass LSTM_3(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(LSTM_3, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.lstm(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create GRU\\nclass GRU_3(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(GRU_3, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.gru(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create LSTM\\nclass LSTM_4(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(LSTM_4, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.lstm(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create GRU\\nclass GRU_4(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(GRU_4, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.gru(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create LSTM\\nclass LSTM_5(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(LSTM_5, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.lstm(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n        \\n        return out\\n\\n# create GRU\\nclass GRU_5(nn.Module):\\n    def __init__(self, input_size, hidden_size, num_layers, num_classes):\\n        super(GRU_5, self).__init__()\\n        self.hidden_size = hidden_size\\n        self.num_layers = num_layers\\n        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\\n        self.fc = nn.Linear(hidden_size*sequence_length, num_classes)\\n\\n    def forward(self, x):\\n        # forward forward prop \\n        out, _ = self.gru(x)\\n        out = out.reshape(out.shape[0], -1)\\n        out = self.fc(out)\\n   '}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm \n",
    "\n",
    "# Disable parallelism in tokenizers\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/tiny_starcoder_py\")\n",
    "\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "# Load the data\n",
    "examples = load_examples_from_json(\"data/code/code.json\")\n",
    "\n",
    "examples = examples[:2]\n",
    "print(examples[0])\n",
    "\n",
    "# Generate completions\n",
    "for example in tqdm(examples):\n",
    "    example[\"generated_middle\"], outputs, generated_text, start_index, end_index = generate_completion(model, tokenizer, model.device, example[\"prefix\"], example[\"suffix\"])\n",
    "\n",
    "print(examples[0])\n",
    "\n",
    "# Optionally, save examples to a JSON file\n",
    "with open(\"data/code/code.json\", \"w\") as f:\n",
    "    json.dump(examples, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "class RNN(nn.Module):\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(generated_text[generated_text.find(\"<fim_middle>\") + len(\"<fim_middle>\"): len(generated_text) - len(\"<|endoftext|>\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_special_tokens({\"pad_token\": \"\\t\"})\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"\\t\"})\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"\\t\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
